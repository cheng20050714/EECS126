{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing and the Neyman-Pearson Lemma\n",
    "\n",
    "#### Authors:\n",
    "v1.0 (Spring 2020) Justin Hong, Christina Zhang, Kannan Ramchandran <br/>\n",
    "v1.1 (Fall 2021) Clark Wang, Shyam Parekh <br/>\n",
    "v1.2 (Spring 2022) Aadil Manazir, Sohom Paul <br/>\n",
    "v1.3 (Fall 2022) Axel Li, Andy Dong, Reina Wang, Kannan Ramchandran <br/>\n",
    "v2.0 (Spring 2023) Andy Dong, Thomas Courtade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Introduction: A Continuous Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a security camera that outputs a signal â€” some random variable $X$. We know that when all is well, the signal will be distributed as standard normal: $X \\sim N(0, 1)$. But if there is an intruder, then the distribution of the signal is shifted, and $X \\sim N(2, 1)$. When we read the signal from the camera, we would like to determine if all is well, or if there is an intruder. If there is an intruder, we would like to correctly determine so. At the same time, we would like to avoid rushing home if there was no intruder after all.\n",
    "\n",
    "This is an example of a binary hypothesis testing problem. There are two possible hypotheses. The first is that all is well, and we can call this $H_0$, the **null hypothesis**. The second is that there is an intruder, and we call this $H_1$, the **alternative hypothesis**. Upon observing $X$, we can either accept the null hypothesis (and reject the alternative hypothesis), or reject the null hypothesis (and accept the alternative hypothesis).\n",
    "\n",
    "We can write a rule for accepting or rejecting the null hypothesis by determining an **acceptance region**. This will be all values of $X$ for which we accept the null hypothesis. An arbitrary example of this could be saying if $X \\in \\{3\\} \\bigcup [6, 9]$, declare that there is no intruder. \n",
    "\n",
    "After we've set an acceptance region, it is possible to raise a false alarm (incorrectly reject the null hypothesis). For example, it is possible that $X \\sim N(0, 1)$, meaning all is well, and $X = 4$, which is outside of the acceptance region. In this case our rule would say that there is an intruder when there is not. The probability of making this type of error is called the **probability of false alarm** (PFA). The PFA is also known as $\\alpha$ or the significance level.\n",
    "\n",
    "$$\\text{PFA} = \\alpha = \\mathbb{P}_{H_0}(\\text{choosing } H_1)$$\n",
    "\n",
    "On the other hand, it could be that there is an intruder, and we detect it correctly (for example,  $X \\sim N(2, 1)$ and $X = 4$). The probability of this is the **probability of correct detection** (PCD), also known as the power. The PCD is also 1 minus the probability of false negative, or $1-\\beta$.\n",
    "\n",
    "$$\\text{PCD} = 1-\\beta = \\mathbb{P}_{H_1}(\\text{choosing } H_1)$$\n",
    "\n",
    "We would like to **maximize PCD** subject to a **limit on PFA**. We can already see from the formulation that there is some sort of trade off. Since both the PFA and the PCD are the probability of $X$ taking on a value in the acceptance region of $H_1$, increasing the acceptance region of $H_1$ increases both the PCD and the PFA. So, the question is how do we choose the acceptance region of $H_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the Hypotheses\n",
    "Let's start by looking at the pdfs of $X$ in each case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = 0\n",
    "mu1 = 2\n",
    "variance = 1\n",
    "sigma = np.sqrt(variance)\n",
    "\n",
    "x_q1 = np.linspace(1 - 5 * sigma, 1 + 5 * sigma, 100)\n",
    "f0 = scipy.stats.norm.pdf(x_q1, mu0, sigma)\n",
    "f1 = scipy.stats.norm.pdf(x_q1, mu1, sigma)\n",
    "fig, (ax1) = plt.subplots(1, 1, sharex=True)\n",
    "\n",
    "ax1.set_title(\"Probability density of X under H0 and H1\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"f(x)\")\n",
    "ax1.plot(x_q1, f0, color=\"red\", label=\"H0\")\n",
    "ax1.plot(x_q1, f1, color=\"blue\", label=\"H1\")\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph above, we have some feeling that if we observe $X < 1$, $H_0$ is more likely, and if we observe $X > 1$, $H_1$ is more likely.\n",
    "\n",
    "Why is this? You may be comparing the conditional probabilities $f_{X|H_1}(x)$ and $f_{X|H_0}(x)$. When $X < 1$, $f_{X|H_0}(x) > f_{X|H_1}(x)$, and when $X > 1$, $f_{X|H_0}(x) < f_{X|H_1}(x)$.\n",
    "Define the **likelihood ratio** as the ratio of these two values. \n",
    "\n",
    "$$ L(x) := \\frac{f_{H_1}(x)}{f_{H_0}(x)} $$\n",
    "\n",
    "Plot the likelihood ratio as a function of $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# use the values calculated in the previous cell to calculate likelihood\n",
    "likelihood = ...\n",
    "### END SOLUTION\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, sharex=True)\n",
    "ax1.set_title(\"Likelihood Ratio\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"Ratio\")\n",
    "ax1.plot(x_q1, likelihood)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the likelihood ratio is mononotonically increasing. The Neyman-Pearson lemma tells us the acceptance region should be the set of $x$ values for which the likelihood ratio is above some threshold that we need to determine. But since the likelihood ratio is monotonically increasing, thresholding on the likelihood ratio is equivalent to thresholding on the $x$ values (for some difference threshold), which looks like\n",
    "\n",
    "1. Accept $H_0$ if $X < c$\n",
    "\n",
    "2. Reject $H_0$ if $X > c$.\n",
    "\n",
    "Because $X$ is continuous, we don't consider $X = c$ here. But that case is important for discrete RVs, as we will see in the next example.\n",
    "\n",
    "Because the likelihood ratio is monotonically increasing, we know that there isn't some high value of $X$, say $c'>c$, beyond which we would change our minds and accept $H_0$ again. In other words, we know that there exists some threshold $c$ for $X$ below which we always accept, and above which we always reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neyman-Pearson Test \n",
    "\n",
    "Let's visualize the acceptance and rejection regions derived from the Neyman-Pearson test. When our likelihood ratio is monotonic, we know the optimal test is a simple threshold decision rule.\n",
    "\n",
    "Calculate the threshold and the PCD for the hypotheses described above. (Hint: Look up norm.cdf and norm.ppf from scipy.stats)\n",
    "\n",
    "Play around with the value of $\\alpha$ and look at the visualization. The x values of the shaded region indicate what values of $X$ we should reject the null hypothesis. The area of the shaded region is the probability of false alarm. How are PFA and PCD related to each other? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Returns the NP threshold for the continuous example above for a given significance level alpha.\n",
    "# Check out scipy.stats.norm documentation to find a suitable function\n",
    "def get_threshold_q1(PFA):\n",
    "    \"\"\"\n",
    "    Given the probability of false alarm, return the \n",
    "    threshold c for which we should reject the null \n",
    "    hypothesis if X > c and accept otherwise.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    # False alarm means choosing H1 under hypothesis H0\n",
    "    # We choose H1 when X > threshold\n",
    "    # PFA = P(N(0, 1) > threshold)\n",
    "\n",
    "    threshold = ...\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return threshold\n",
    "\n",
    "\n",
    "# Returns the probability of correct detection for the continuous example above\n",
    "# given a threshold (for a decision rule).\n",
    "# Check out scipy.stats.norm documentation to find a suitable function\n",
    "def get_pcd_q1(threshold):\n",
    "    \"\"\"\n",
    "    Calculate the probability of correct detection given the alternate \n",
    "    hypothesis is true for this particular threshold value.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # We choose H1 when X > threshold\n",
    "    # PCD = P(N(2, 1) > threshold)\n",
    "\n",
    "    pcd = ...\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_continuous_geq_threshold(x_q1, f0, f1, get_threshold_q1, get_pcd_q1, interactive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How does PFA change with $\\alpha$? How does PCD change with $\\alpha$?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How would the acceptance region look if we had a monotonically decreasing likelihood ratio rather than a monotonically increasing one?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Error Curve\n",
    "Now, we plot the $\\alpha$ vs PFN curve (where PFN is probability of false negative, PFN = $1-$ PCD) of the optimal decision rules to visualize the tradeoff. In the original setting, we want to maximize PCD which is equivalent to minimizing PFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the PFN value of 300\n",
    "# different values of PFA then plot them\n",
    "alphas_q1 = np.linspace(0, 1, 300)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "PFNs_q1 = ...\n",
    "### END SOLUTION\n",
    "\n",
    "plt.title('PFA vs PFN Tradeoff Curve')\n",
    "plt.plot(PFNs_q1, alphas_q1)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Probability of False Negative')\n",
    "plt.ylabel('Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens to PFN (and PCD) if we want a very small PFA? What would be the decision rule if our constraint on PFA is PFA $=0$?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What part of the graph is the set of all Neyman-Pearson tests? What part of the graph is the set of all possible tests? What part of the graph is unachievable sets of (PFA, PFN) pairs?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our error tradeoff curve is a smooth curve. This will not be the case for discrete observation variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: A Discrete Example with Deterministic Decision Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider a discrete example. \n",
    "\n",
    "We observe $X \\sim Bin(n,p)$, where $n$ is the same under both hypotheses but we wish to test $H_0: p=p_0$ vs $H_1: p=p_1$, with $p_0 < p_1$.\n",
    "\n",
    "Implement the Neyman-Pearson test for discrete distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants.\n",
    "n = 6\n",
    "p0 = 0.3\n",
    "p1 = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_q2 = np.arange(n + 1)\n",
    "p0 = scipy.stats.binom.pmf(x_q2, n, p0)\n",
    "p1 = scipy.stats.binom.pmf(x_q2, n, p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, sharex=True)\n",
    "ax1.set_title(\"Likelihood ratio\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"L(x)\")\n",
    "ax1.scatter(x_q2, p1 / p0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that again this likelihood ratio is monotonically increasing, meaning that we can threshold on $x$ values instead of likelihood ratios $L(x)$ like in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the below function for a **deterministic** (and non-optimal) decision rule that thresholds on $x$ values. Note that for the deterministic rule for discrete $X$, PFA and $\\alpha$ are no longer the same. $\\alpha$ is the provided upper-bound on PFA, and PFA may not reach $\\alpha$. Later, we will implement the optimal, randomized decision rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_q2(p0, alpha):\n",
    "    # Returns a threshold c such that under p0,\n",
    "    # P(X > c) <= alpha; P(X >= c) > alpha\n",
    "    # p0 is the array [P(X=0), P(X=1), ..., P(X=n)] under p0\n",
    "    \n",
    "    threshold = len(p0) - 1\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcd_q2(p1, threshold):\n",
    "    # Returns the PCD, where threshold is returned by the previous function\n",
    "    PCD = 0\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return PCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Error Curve for the Deterministic Decision Rule\n",
    "Similarly, we plot the $\\alpha$ vs PFN curve. Before you write and run the code, can you predict what the graph will look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_q2 = np.linspace(0, 1, 300)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "PFNs_q2 = ...\n",
    "### END SOLUTION\n",
    "\n",
    "plt.title('PFA vs PFN Tradeoff Curve')\n",
    "plt.plot(PFNs_q2, alphas_q2)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Probability of False Negative')\n",
    "plt.ylabel('Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why does the curve look like a step function? Why is it that sometimes when we decrease $\\alpha$, our PFN sees a sudden increase? How does a randomized decision rule solve this problem?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: At what points in the graph above is the deterministic rule optimal, if there is any, and why?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: The Randomized Decision Rule\n",
    "Time for the boss! Under the same discrete setting as above, implement the following functions for the randomized Neyman-Pearson decision rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_and_randomization_q3(p0, alpha):\n",
    "    # Returns a tuple (threshold, randomization_constant)\n",
    "    # Returns a threshold c such that under p0,\n",
    "    # P(X > c) <= alpha; P(X >= c) > alpha\n",
    "    # Returns a randomization constant gamma such that under p0,\n",
    "    # P(X > c) + gamma * P(X = c) = alpha\n",
    "    # p0 is the array [P(X=0), P(X=1), ..., P(X=n)] under p0\n",
    "    \n",
    "    threshold = len(p0) - 1\n",
    "    gamma = 0\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return threshold, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcd_q3(p1, threshold, gamma):\n",
    "    # Returns the PCD\n",
    "    # where threshold and gamme are returned by the previous function\n",
    "    PCD = 0\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return PCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Error Curve for the Deterministic Decision Rule\n",
    "Similarly, we plot the $\\alpha$ vs PFN curve. Before you write and run the code, can you predict what the graph will look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_q3 = np.linspace(0, 1, 300)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "PFNs_q3 = ...\n",
    "### END SOLUTION\n",
    "\n",
    "plt.title('PFA vs PFN Tradeoff Curve')\n",
    "plt.plot(PFNs_q3, alphas_q3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Probability of False Negative')\n",
    "plt.ylabel('Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What parts of the curve are deterministic decision rules, and what parts are randomized decision rules?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the error curve of deterministic decision rules and of randomized decision rules on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('PFA vs PFN Tradeoff Curve')\n",
    "plt.plot(PFNs_q2, alphas_q2, label='deterministic')\n",
    "plt.plot(PFNs_q3, alphas_q3, label='randomized')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Probability of False Negative')\n",
    "plt.ylabel('Alpha')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that under the same $\\alpha$, the randomized rule gives a lower PNF. Similarly, under the same PNF, the randomized rule also gives a lower $\\alpha$, so the randomized rule performs at least as good as the deterministic rule. In fact, in lecture, you learned that the Neyman-Pearson decision rule is the **optimal** rule among all. The points where the two error curves meet are the ($\\alpha$, PNF) value pairs such that the randomization constant $\\gamma$ is 0. The randomized rule is constructed by taking convex combinations of these points (where a convex combination is a linear combination with nonnegative weights that sum to 1), and the convex combination mixture weights are $[\\gamma, 1-\\gamma]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
