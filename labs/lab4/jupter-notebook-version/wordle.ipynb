{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordle!\n",
    "\n",
    "#### Authors:\n",
    "v1.0 (Fall 2022): Andy Dong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This version of the lab is designed to run locally on your machine. Please do not use JupyterHub to run this version as the lab requires more memory than allowed by JupyterHub. If you are unable to run this lab locally, please check out the Google Colab version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the game of [Wordle](https://www.nytimes.com/games/wordle/index.html), your goal is to deduce a 5-letter secret word that the game has generated using as few guesses as possible. After each guess, the game will tell you for each letter in your guess, whether the letter is\n",
    "\n",
    "-  in the secret word and in the right spot (indicated by green),\n",
    "-  in the secret word but in the wrong spot (indicated by yellow),\n",
    "-  or not in the secret word (indicated by gray).\n",
    "\n",
    "Take a look at the example game below where the secret word is \"scald\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"examples/ex-0.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andy wants to write a program that solves the problem as optimally as possible. In this lab, you will explore how to write a greedy algorithm using Information Theory. It's recommended that you try a game of Wordle to familiarize yourself with the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the Game\n",
    "\n",
    "Here's some setup and simplifying assumptions about the game. First, we have obtained the list of 2315 words that can be chosen as the secret word. The secret word $X$ is modeled to be a random variable that has uniform distribution over these possible words. Second, your guess cannot be any arbitrary combination of 5 letters but needs to be from a list of 12972 valid guesses (which, of course, contains the 2315 possible secret words). Third, any revealed hints don't need to be used in subsequent guesses. With these, let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to read in the list of possible secret words and the list of allowed guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/possible_words.txt') as file:\n",
    "    possible_words = [line.rstrip() for line in file]\n",
    "with open('data/allowed_guesses.txt') as file:\n",
    "    allowed_guesses = [line.rstrip() for line in file]\n",
    "\n",
    "print('Number of possible_words:', len(possible_words))\n",
    "print('Number of allowed_guesses:', len(allowed_guesses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out some of these words to take a look. `possible_words` consists of words used more frequently in the English language while `allowed_guesses` constains some words that we don't see in day-to-day life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Some possible secret words:')\n",
    "print(*random.sample(possible_words, 20), '\\n')\n",
    "print('Some allowed guesses:')\n",
    "print(*random.sample(allowed_guesses, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** The secret word is picked uniformly at random from 2315 possible words. What is the entropy of that distribution in unit of bits? And what does it mean about how many bits of information we need to gain to recover the secret word?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Game Logic (Read Only)\n",
    "\n",
    "We then implement the logic of the game by first computing the pattern that we will see for a given pair of guess and secret word. The function `compute_pattern` takes in a guess and a secret word, and returns an integer tuple of length 5 that represents the color pattern, where an entry of 2 represents green in the corresponding position, 1 represents yellow, and 0 represents gray.\n",
    "\n",
    "There's an edge case that we will need to consider, which is when the guess and/or the secret word contains multiple of the same letter. The rule here is that green (match in the correct spot) always has the highest priority, yellow (match in the wrong spot) prioritizes letters in earlier positions of your guess, and each letter in the secret word can only correspond to one letter in the guess. Take a look at the example below.\n",
    "\n",
    "- If the secret word is \"abide\" and the guess is \"three\", the pattern would be (0, 0, 0, 0, 2). Position 5 is an exact match so it has the highest priority to be green. Position 4 is indeed a match in the wrong spot, but since the \"e\" in \"abide\" is already taken, it cannot match to that same \"e\" again.\n",
    "\n",
    "![title](examples/ex-1.png)\n",
    "\n",
    "- If the secret word is \"abide\" and the guess is \"drama\", the pattern would be (1, 0, 1, 0, 0). Position 3 is a yellow match that has priority over the potential yellow match at position 5.\n",
    "\n",
    "![title](examples/ex-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pattern(guess, answer):\n",
    "    # Returns a length 5 tuple\n",
    "    \n",
    "    pattern = [0, 0, 0, 0, 0]\n",
    "    taken = [False, False, False, False, False]\n",
    "    \n",
    "    # Green pass\n",
    "    for i in range(5):\n",
    "        if guess[i] == answer[i]:\n",
    "            # If it's an exact match, color it green, and mark it as taken\n",
    "            # so that the yellow pass doesn't match to it again\n",
    "            pattern[i] = 2\n",
    "            taken[i] = True\n",
    "    \n",
    "    # Yellow pass\n",
    "    for i in range(5):\n",
    "        if pattern[i] == 2:\n",
    "            # If a spot is already colored green, we skip it\n",
    "            continue\n",
    "        query = guess[i]\n",
    "        for j in range(5):\n",
    "            if query == answer[j] and taken[j] is False:\n",
    "                # If there is a misplaced match that is not taken by the\n",
    "                # green pass or a previous yellow pass, we color it yellow\n",
    "                # and mark it as taken\n",
    "                pattern[i] = 1\n",
    "                taken[j] = True\n",
    "                break\n",
    "    \n",
    "    return tuple(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the pattern between all pairs of potential guesses and potential answers, and tabulate the results for faster lookups in the future. This will take a while to run but save you a massive amount of time later. In later parts of the lab, you should use `pattern_table[guess][answer]` instead of `compute_pattern(guess, answer)`.\n",
    "\n",
    "*Note*: Since this code block takes a long time to run, we also store the result in a file. If you don't complete the lab in one sitting, you can come back to it by loading the stored dictionary instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_file = False\n",
    "\n",
    "pattern_table = {}\n",
    "\n",
    "if not load_from_file:\n",
    "    for guess in tqdm(allowed_guesses):\n",
    "        word_table = {}\n",
    "        for answer in possible_words:\n",
    "            word_table[answer] = compute_pattern(guess, answer)\n",
    "        pattern_table[guess] = word_table\n",
    "\n",
    "    if not os.path.exists('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    file = open('checkpoint/pattern_table', 'ab')\n",
    "    pickle.dump(pattern_table, file)\n",
    "    file.close()\n",
    "else:\n",
    "    file = open('checkpoint/pattern_table', 'rb')\n",
    "    pattern_table = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess Strategy\n",
    "\n",
    "We use the term \"alphabet of a random variable\" to mean the set of potential values it could take on with positive probability (not to be confused with the English alphabet). For example, the alphabet of $X$ is the set of 2315 possible secret words.\n",
    "\n",
    "Fix a time index $t$. Let $X_t = X|Y_1, \\ldots, Y_{t-1}$ where $Y_i$ is the pattern we observe after our $i$th guess, which we assume has already happened. Then, we can see that $X_t$ also has a uniform distribution over its alphabet because the color pattens we observe only tell us which secret words are possible and which are not, but do not change the relative probabilities assigned to the possible secret words. In other words, we can think of each guess as \"narrowing down\" the alphabet of $X$.\n",
    "\n",
    "Let $Y_{t,k}$ be the resulting pattern for guessing the word $k$ at timestep $t$. Both $X_t$ and $Y_{t,k}$ are random variables. Note that the index $k$ here is a word we guess. We want to minimize the conditional entropy $H(X_t|Y_{t,k})$ over $k \\in$ `allowed_guesses` since it's the \"leftover uncertainty\" about $X_t$ after observing the color pattern for $k$. Namely, if $Y_t=Y_{t,k}$ (meaning that we guess word $k$), then $H(X_t|Y_{t,k})=\\log_2 |\\text{alphabet of $X_{t+1}$}|$ since $X_{t+1}$ is still uniform over its alphabet.\n",
    "\n",
    "Recall that $$H(X_t)=I(X_t;Y_{t,k})+H(X_t|Y_{t,k}).$$\n",
    "\n",
    "Since $H(X_t)$ is a constant given a particular observation of $Y_1, \\ldots, Y_{t-1} = (y_1, \\ldots, y_{t-1})$, minimizing $H(X_t|Y_{t,k})$ is equivalent to maximizing $I(X_t;Y_{t,k})$.\n",
    "\n",
    "But what is $I(X_t;Y_{t,k})$? The mutual information is the amount of information in $X_t$ gained through observing $Y_{t,k}$, which equals the amount of information in $Y_{t,k}$ gained through observing $X_t$. However, if we know $X_t$ (which means we peek at the answer), then $Y_{t,k}|X_t$ is deterministic! Knowing $X_t$ would reduce the uncertainty in $Y_{t,k}$ from $H(Y_{t,k})$ to $0$, which means that\n",
    "\n",
    "$$I(X_t;Y_{t,k})=H(Y_{t,k})-H(Y_{t,k}|X_t)=H(Y_{t,k})-0=H(Y_{t,k}).$$\n",
    "\n",
    "The conclusion is that we reformulate the problem of minimizing $H(X_t|Y_{t,k})$ into maximizing $I(X_t;Y_{t,k})$ then into maximizing $H(Y_{t,k})$. Please make sure you fully understand our steps above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start by implementating `divide_alphabet`**, which takes in a guess and the current alphabet. The function would split the alphabet into smaller subgroups. Namely, it returns a dictionary that maps from the set of possible color patterns to the set of secret words such that `pattern_table[guess][secret_word]` is that color pattern. For example, if `guess` is \"shake\" and `alphabet` is {shape, shake, shame}, then the function should return the mapping (2,2,2,2,2)$\\to${shake}, (2,2,2,0,2)$\\to${shape, shame}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_alphabet(guess, alphabet):\n",
    "    pattern_to_subgroup = {}\n",
    "    # TODO: your code here\n",
    "    \n",
    "    return pattern_to_subgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, since $X$ is uniform over its alphabet, the probability that we observe each pattern is proportional to the number of words in that subgroup. **In the following cell, implement `prob_dist`**, which takes in the output of the above function and returns the probability distribution over the set of possible patterns. For the same example above, this function would take in the mapping (2,2,2,2,2)$\\to${shake}, (2,2,2,0,2)$\\to${shape, shame} and return $\\left[\\frac{1}{3}, \\frac{2}{3}\\right]$ This is the distribution of $Y_{t,k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_dist(pattern_groups):\n",
    "    # returns a probability distribution in the form of a list\n",
    "    # for example, if the probability distribution is\n",
    "    # P(pattern_1) = 0.2, P(pattern_2) = 0.3, P(pattern_3) = 0.5,\n",
    "    # this function should return [0.2, 0.3, 0.5]. Order doesn't matter\n",
    "    dist = []\n",
    "    # TODO: your code here\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the cell below, implement the function `entropy`**, which takes in a probability distribution (in a list format, like the output of the previous function) and outputs its entropy. You may assume that the distribution has no entry of $0$ and is a valid distribution. From this we can find $H(Y_k)$, which is the quantity we seek to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(dist):\n",
    "    # TODO: your code here\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're into business! We've specified how to quantitatively compare quality of guesses, and **let's now implement `find_best_guess`**, which takes in the alphabet of $X_t$ and returns the best guess to make. Recall that the best guess is a guess $k\\in$ `allowed_guesses` that maximizes $H(Y_{t,k})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_guess(alphabet, allowed_guesses, use_tqdm=False):\n",
    "    word, highest = None, 0\n",
    "    if use_tqdm:\n",
    "        allowed_guesses = tqdm(allowed_guesses)  # tqdm shows the progress bar\n",
    "    for guess in allowed_guesses:\n",
    "        # TODO: your code here\n",
    "        \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Wordle Opener\n",
    "\n",
    "Here is the question that Wordle lovers are looking for: What is the best opening guess under our scheme? (**Important Note:** this is the best opener given our assumptions that the secret word is picked *uniformly at random* over the 2315 possible secret words, and we are using a greedy algorithm to get the most information out of each guess. This is not guaranteed to be the truly optimal guess.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_opener = find_best_guess(possible_words, allowed_guesses, use_tqdm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best Wordle opener is:', best_opener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer to the previous output is \"soare\", which is an obsolete form of sore (“A young hawk”) by Collins dictionary. Let's plot out the distribution of patterns we observe if we open with \"soare\", then compare it with if we opened with \"speed\" — a mediocre opener, and \"qajaq\" (which is an alternative spelling of kayak) — the worst opener. Since there's no natural ordering to pattern, we will instead order the patterns on the x-axis by decreasing probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = prob_dist(divide_alphabet('soare', possible_words))\n",
    "print(f'Expected information gain: {round(entropy(distribution),2)} bits out of 11.18 bits')\n",
    "distribution.sort(reverse=True)\n",
    "patterns = list(range(1, len(distribution) + 1))\n",
    "plt.bar(patterns, distribution)\n",
    "plt.title('Distribution of Patterns for SOARE Opening')\n",
    "plt.xlabel('Pattern Number')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = prob_dist(divide_alphabet('speed', possible_words))\n",
    "print(f'Expected information gain: {round(entropy(distribution),2)} bits out of 11.18 bits')\n",
    "distribution.sort(reverse=True)\n",
    "patterns = list(range(1, len(distribution) + 1))\n",
    "plt.bar(patterns, distribution)\n",
    "plt.title('Distribution of Patterns for SPEED Opening')\n",
    "plt.xlabel('Pattern Number')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = prob_dist(divide_alphabet('qajaq', possible_words))\n",
    "print(f'Expected information gain: {round(entropy(distribution),2)} bits out of 11.18 bits')\n",
    "distribution.sort(reverse=True)\n",
    "patterns = list(range(1, len(distribution) + 1))\n",
    "plt.bar(patterns, distribution)\n",
    "plt.title('Distribution of Patterns for QAJAQ Opening')\n",
    "plt.xlabel('Pattern Number')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"qajaq\" opener is so bad that only a few patterns have a positive probability of appearing, which has a very low entropy. Note that the horizontal axis of the last graph is on a different scale from the previous graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** From how the graphs look, which one has a higher entropy and why? And why does it suggest \"soare\" is a better opener than \"speed\"?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Optimizations (Read Only)\n",
    "\n",
    "This part has no implementation that you need to do, but it's highly recommended that you read through this section.\n",
    "\n",
    "The calculation to find \"soare\" took a while to run, didn't it? You may realize that the opening guess is always made without any observed pattern, so we don't need to calculate it every time, but can instead save the result that if it's our first guess, we would always return \"soare\".\n",
    "\n",
    "Another thing is that now our guesses always come from the huge list of allowed guesses. Suppose that we have now narrowed the alphabet of $X_t$ down to 2 words, \"price\" and \"pride\". Guessing a word that isn't in the alphabet, such as \"camel\", will certainly tell us which one is the correct answer (since it tells us whether there is a \"c\" in the answer), but we will still need to use a guess to report the correct answer, for a total of 2 additional guesses. However, if we guess a word in the alphabet, not only will it tell us which one of \"price\" and \"pride\" is the answer, but also we have a $\\frac{1}{2}$ probability to hit it right there so we don't need another guess. Therefore, when the remaining alphabet is small ($\\leq 3$), we can instead limit our guesses to within the alphabet.\n",
    "\n",
    "The following function implements these optimization ideas. Take a look at it to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_guess_optimized(alphabet):\n",
    "    if len(alphabet) == 2315:\n",
    "        # if it's the opening guess, we directly output 'soare'\n",
    "        return 'soare'\n",
    "    elif len(alphabet) == 1:\n",
    "        # if we are certain what the secret word is, directly guess it\n",
    "        return alphabet[0]\n",
    "    elif len(alphabet) <= 3:\n",
    "        # if the alphabet is small, limit our guess to within the alphabet\n",
    "        return find_best_guess(alphabet, alphabet)\n",
    "    else:\n",
    "        # otherwise, we apply no optimization\n",
    "        return find_best_guess(alphabet, allowed_guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Test\n",
    "\n",
    "Let's now sample some secret words and watch our algorithm find the secret word in action! **Implement the function `play_wordle`**, which takes in the true secret word and simulates a Wordle game using our algorithm above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordle_game(true_answer):\n",
    "    def wordle_game(guess):\n",
    "        # takes in a guess and outputs the pattern\n",
    "        return pattern_table[guess][true_answer]\n",
    "    \n",
    "    return wordle_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_wordle(wordle_game, print_guesses=False):\n",
    "    alphabet = possible_words\n",
    "    num_guesses = 0\n",
    "    while True:\n",
    "        num_guesses += 1\n",
    "        guess = find_best_guess_optimized(alphabet)\n",
    "        color_pattern = wordle_game(guess)\n",
    "        if print_guesses:\n",
    "            print(f'Guess {num_guesses}: {guess}  |  Pattern: {color_pattern}')\n",
    "        if color_pattern == (2, 2, 2, 2, 2):\n",
    "            # correct answer!\n",
    "            break\n",
    "        # TODO: find the true pattern observed, and then update alphabet\n",
    "        \n",
    "\n",
    "    return num_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    true_answer = random.choice(possible_words)\n",
    "    print('Secret word:', true_answer)\n",
    "    wordle_game = create_wordle_game(true_answer)\n",
    "    play_wordle(wordle_game, print_guesses=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of the number of guesses needed by sampling 300 secret words. If it takes way too long to run, you can instead sample fewer secret words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_guesses = []\n",
    "num_samples = 300\n",
    "for true_answer in tqdm(random.sample(possible_words, num_samples)):\n",
    "    num_guesses.append(play_wordle(create_wordle_game(true_answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = list(range(1, 8))\n",
    "plt.hist(num_guesses, bins, alpha=0.5, edgecolor='black', align='left')\n",
    "plt.xlabel('Number of Guesses')\n",
    "plt.xticks(list(range(1, 8)))\n",
    "plt.title('Wordle Solver Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, your Wordle solver takes fewer guesses on average than American players on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average number of Wordle guesses using entropy: {round(sum(num_guesses)/num_samples, 3)}')\n",
    "print('Average number of Wordle guesses for Americans: 3.92')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordle Bot! (Optional)\n",
    "\n",
    "For the final part of the lab, let's use what we've built so far and transform it into a Wordle bot! **Implement the following class `WordleBot`** that interactively outputs its suggested guess, and receives inputs that consist of your actual guess and the pattern displayed. Feel free to modify the code below to tailor the bot to your liking, such as outputting more than one suggested word, outputting the word along with its mutual information with the secret word, or giving it a better user interface in your personal project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordleBot:\n",
    "    def __init__(self):\n",
    "        # initialize for a new game\n",
    "        self.alphabet = possible_words\n",
    "        self.suggest()\n",
    "    \n",
    "    def suggest(self):\n",
    "        # TODO: when called, the bot gives you the best word to guess\n",
    "        suggested_guess = None\n",
    "        print('Next word to guess:', suggested_guess)\n",
    "    \n",
    "    def observe(self, word, pattern):\n",
    "        # after a guess, feed the pattern to the bot to update\n",
    "        # then, the bot suggests a word to guess\n",
    "        assert len(word) == len(pattern) == 5\n",
    "        # TODO: update self.alphabet according to the observation,\n",
    "        # then call self.suggest()\n",
    "        \n",
    "        self.suggest()\n",
    "    \n",
    "    def restart(self):\n",
    "        # TODO: re-initialize the bot for a new game\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = WordleBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.observe('soare', (0, 1, 0, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.observe('cutin', (2, 0, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.observe('crock', (2, 2, 2, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
